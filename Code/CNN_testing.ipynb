{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "106e15a0",
   "metadata": {},
   "source": [
    "### Convolutional Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5db209a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "\n",
    "# Seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02856ece",
   "metadata": {},
   "source": [
    "### Plot formatting wrt revtex4 in LaTeX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78efc69e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "\n",
    "# Widths in inches from revtex4's layout\n",
    "# Single column ~3.375in, double column ~7in\n",
    "columnwidth = 3.375  # use 7.0 for two-column-wide figures\n",
    "\n",
    "# Compute figure size (width, height)\n",
    "fig_width = columnwidth\n",
    "fig_height = columnwidth / 1.618  # golden ratio for aesthetics\n",
    "fig_size = [fig_width, fig_height]\n",
    "\n",
    "mpl.rcParams.update({\n",
    "    # Use LaTeX for text rendering\n",
    "    \"text.usetex\": True,\n",
    "    \"font.family\": \"serif\",\n",
    "    \"font.serif\": [],  # empty means use LaTeX default (Computer Modern)\n",
    "    \"font.size\": 10.0,  # matches REVTeX's \\normalsize\n",
    "\n",
    "    # Adjust tick and label sizes\n",
    "    \"axes.labelsize\": 10.0,\n",
    "    \"legend.fontsize\": 8.0,\n",
    "    \"xtick.labelsize\": 8.0,\n",
    "    \"ytick.labelsize\": 8.0,\n",
    "\n",
    "    # Figure dimensions\n",
    "    \"figure.figsize\": fig_size,\n",
    "\n",
    "    # Save with good resolution\n",
    "    \"savefig.dpi\": 300,\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b70050d",
   "metadata": {},
   "source": [
    "Fetching data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38bcbc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean and standard deviation from data.ipynb\n",
    "mean = [0.5]\n",
    "std = [0.5]\n",
    "\n",
    "# Resizing images to 480x480 pixels\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((480, 480)),\n",
    "    transforms.RandomHorizontalFlip(p=0.5),\n",
    "    transforms.RandomRotation(10),\n",
    "    transforms.ColorJitter(brightness=0.3),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "val_test_transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((480, 480)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=mean, std=std)])\n",
    "\n",
    "# Dataset\n",
    "dataset_path = \"chest_xray_data_split\"\n",
    "train_dataset = datasets.ImageFolder(f\"{dataset_path}/train\", transform=train_transform)\n",
    "val_dataset   = datasets.ImageFolder(f\"{dataset_path}/val\", transform=val_test_transform)\n",
    "test_dataset  = datasets.ImageFolder(f\"{dataset_path}/test\", transform=val_test_transform)\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader   = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "classes = train_dataset.classes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705e9d47",
   "metadata": {},
   "source": [
    "Model initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc8b14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters\n",
    "num_classes = 2\n",
    "num_epochs = 10\n",
    "batch_size = 32 \n",
    "\n",
    "# CNN model. Inspiration from https://www.datacamp.com/tutorial/pytorch-cnn-tutorial\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self, padding=1, stride=2, kernel_size=3, pool_kernel=3, pool_stride=2):\n",
    "        super().__init__()\n",
    "\n",
    "        ## Convolutional layer\n",
    "        # 1 input, 16 output features\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size, stride, padding)\n",
    "        # 16 input features, 32 output features\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size, stride, padding)\n",
    "        # 32 input features, 64 output features\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size, stride, padding)\n",
    "\n",
    "        # Pooling layer - same configuration for all pooling layers\n",
    "        self.pool = nn.MaxPool2d(pool_kernel, pool_stride)\n",
    "\n",
    "        # Calculates the size of first fully connected layer \n",
    "        with torch.no_grad():\n",
    "            input = torch.zeros(1,1,480,480)\n",
    "            feature1_size = self.pool(F.relu(self.conv1(input)))\n",
    "            feature2_size = self.pool(F.relu(self.conv2(feature1_size)))\n",
    "            feature3_size = self.pool(F.relu(self.conv3(feature2_size)))\n",
    "            self.flatten_size = feature3_size.numel()\n",
    "\n",
    "        # 2 Fully connected layer\n",
    "        self.fc1 = nn.Linear(self.flatten_size, 32)\n",
    "        self.fc2 = nn.Linear(32, num_classes-1)\n",
    "\n",
    "    # Forward pass of the neural network.\n",
    "    def forward(self, x):\n",
    "        # x is the input tensor8\n",
    "        x = F.relu(self.conv1(x)) # First convolutional layer with ReLU activation\n",
    "        x = self.pool(x) # First max pooling layer\n",
    "        x = F.relu(self.conv2(x)) # Second convolutionial layer with ReLU activation\n",
    "        x = self.pool(x) # First max pooling layer\n",
    "        x = F.relu(self.conv3(x)) # Third convolutionial layer with ReLU activation\n",
    "        x = self.pool(x) # Third max pooling third\n",
    "        x = x.view(x.size(0), -1) # flattening\n",
    "        x = F.relu(self.fc1(x)) # fully connected layer\n",
    "        return self.fc2(x)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e390be",
   "metadata": {},
   "source": [
    "Accuracy function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a78c3ab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(logits, y):\n",
    "    # 0.5 is the threshold value that determines what is classificed as a 1\n",
    "    prediction = (torch.sigmoid(logits) >= 0.5).float()\n",
    "    correct = (prediction == y).float().sum()\n",
    "    return correct / y.numel()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cffc3eb",
   "metadata": {},
   "source": [
    "Training function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd09957",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, val_loader, num_epochs=20, eta=1e-3,device=\"cpu\"):\n",
    "\n",
    "    model.to(device)\n",
    "    cost_fn = nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=eta)\n",
    "\n",
    "    # History lists for plotting\n",
    "    train_cost = []\n",
    "    val_cost = []\n",
    "    train_acc = []\n",
    "    val_acc = []\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_train_cost = 0\n",
    "        epoch_train_acc  = 0\n",
    "\n",
    "        # tqdm used to display estimated time per epoch\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\", leave=False):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device).float().reshape(-1, 1)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits = model(x)\n",
    "            cost   = cost_fn(logits, y)\n",
    "\n",
    "            cost.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            epoch_train_cost += cost.item()\n",
    "            epoch_train_acc  += accuracy(logits, y).item()\n",
    "\n",
    "        # Average training batch cost and accuracy\n",
    "        avg_train_cost = epoch_train_cost / len(train_loader)\n",
    "        avg_train_acc  = epoch_train_acc  / len(train_loader)\n",
    "\n",
    "        # Validation data\n",
    "        model.eval()\n",
    "        epoch_val_cost = 0\n",
    "        epoch_val_acc  = 0\n",
    "\n",
    "        # Generating predictions on the validation data \n",
    "        with torch.no_grad():\n",
    "            for x, y in val_loader:\n",
    "                x = x.to(device)\n",
    "                y = y.to(device).float().reshape(-1, 1)\n",
    "\n",
    "                logits = model(x)\n",
    "                cost   = cost_fn(logits, y)\n",
    "\n",
    "                epoch_val_cost += cost.item()\n",
    "                epoch_val_acc  += accuracy(logits, y).item()\n",
    "\n",
    "        # Average validation batch cost and accuracy\n",
    "        avg_val_cost = epoch_val_cost / len(val_loader)\n",
    "        avg_val_acc  = epoch_val_acc  / len(val_loader)\n",
    "\n",
    "        # Save history\n",
    "        train_cost.append(avg_train_cost)\n",
    "        val_cost.append(avg_val_cost)\n",
    "        train_acc.append(avg_train_acc)\n",
    "        val_acc.append(avg_val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} Train cost: {avg_train_cost:.4f} Validation cost: {avg_val_cost:.4f} Validation accuracy: {avg_val_acc:.4f}\")\n",
    "\n",
    "    return {\"train_cost\": train_cost,\n",
    "            \"val_cost\":   val_cost,\n",
    "            \"train_acc\":  train_acc,\n",
    "            \"val_acc\":    val_acc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6e1d82",
   "metadata": {},
   "source": [
    "#### Hyperparameterizing filter size - convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643e8453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameter\n",
    "kernel_sizes = [2,3,4,5]\n",
    "\n",
    "kernel_results = []\n",
    "kernel_metrics = []\n",
    "\n",
    "# Iterating over the hyperparameters\n",
    "for size in kernel_sizes:\n",
    "        # Initializing CNN with hyperparameters\n",
    "        model = CNN(kernel_size=size)  \n",
    "        # Metrics from trained model\n",
    "        kernel_conv_layer = train_model(model, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "        # Store metrics for the current hyperparameter combination\n",
    "        kernel_metrics.append({\"kernel_size\": size, \"history\": kernel_conv_layer})\n",
    "\n",
    "        # Evaluating the current hyperparameter combination\n",
    "        model.eval()\n",
    "\n",
    "        cost_accumulated = 0.0\n",
    "        n_samples = 0\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # Generating predictions on the test data \n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                logits = model(x)\n",
    "\n",
    "                # BCEWithLogitsLoss expects float (1.0, 0.0)\n",
    "                y = y.float().unsqueeze(1)\n",
    "\n",
    "                current_cost = torch.nn.BCEWithLogitsLoss()(logits, y)\n",
    "\n",
    "                cost_accumulated += current_cost.item() * x.size(0)\n",
    "                n_samples += x.size(0)\n",
    "\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= 0.5).int()\n",
    "\n",
    "                y_true.extend(y.squeeze(1).int().numpy())\n",
    "                y_pred.extend(preds.numpy())\n",
    "\n",
    "        # Average test_cost\n",
    "        test_cost = cost_accumulated / n_samples\n",
    "\n",
    "        # Calculating the TN, FP, FN and TP\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Calculating sensitivity and specificity\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "        # Storing results\n",
    "        kernel_results.append({\n",
    "            \"kernel_size\": size,\n",
    "            \"test_cost\": test_cost,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"sensitivity\": sensitivity,\n",
    "            \"specificity\": specificity})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4855d547",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_sizes = sorted([r[\"kernel_size\"] for r in kernel_results])\n",
    "kernel_costs = [r[\"test_cost\"] for r in kernel_results]\n",
    "\n",
    "# Initialize matrices\n",
    "cost_matrix_kernel = np.array(kernel_costs).reshape(1, -1)\n",
    "combined_matrix = np.zeros((2, len(kernel_sizes)))\n",
    "\n",
    "# Formatting labels to display \"2x2\", \"3x3\", ...\n",
    "kernel_labels = [f\"{k}x{k}\" for k in kernel_sizes]\n",
    "\n",
    "# Combining sensitivity and specificity results into one matrix for plotting\n",
    "for j, r in enumerate(kernel_results):\n",
    "    combined_matrix[0, j] = r[\"sensitivity\"]\n",
    "    combined_matrix[1, j] = r[\"specificity\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996fe5f7",
   "metadata": {},
   "source": [
    "#### Cost - Filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35abf54c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(cost_matrix_kernel,annot=True,fmt=\".4f\",cmap=\"Greens\",xticklabels=kernel_labels,yticklabels=[\" \"])\n",
    "plt.xlabel(\"Filter Size\")\n",
    "plt.title(\"Binary Cross-Entropy - Conv layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "507cf833",
   "metadata": {},
   "source": [
    "#### Sensitivity & specificity - Filter size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310cb538",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(combined_matrix,annot=True,fmt=\".3f\",cmap=\"Greens\",xticklabels=kernel_labels,yticklabels=[\"Sensitivity\", \"Specificity\"])\n",
    "plt.xlabel(\"Filter Size\")\n",
    "plt.title(\"Sensitivity and Specificity\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca7e1c5",
   "metadata": {},
   "source": [
    "### Hyperparameterizing padding and stride - Convolutional layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a264f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "padding = [0,1,2]\n",
    "strides = [1,2,3]\n",
    "epochs = 10\n",
    "\n",
    "# Initialzing lists to store evaluation metrics across models\n",
    "results = []\n",
    "conv_metrics = []\n",
    "\n",
    "# Iterating over the hyperparameters\n",
    "for pad in padding:\n",
    "    for stride in strides:\n",
    "        # Initializing CNN with hyperparameters\n",
    "        model = CNN(padding=pad, stride=stride)\n",
    "        # Metrics from trained models\n",
    "        convolutional_layer = train_model(model,train_loader,val_loader,num_epochs=epochs)\n",
    "        # Store metrics for the current hyperparameter combination\n",
    "        conv_metrics.append({\"padding\": pad,\"stride\": stride,\"history\": convolutional_layer})\n",
    "\n",
    "        # Evaluate the trained model\n",
    "        model.eval()\n",
    "        \n",
    "        cost_accumulated = 0.0\n",
    "        n_samples = 0\n",
    "        \n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "\n",
    "        # Generating predictions on the test data \n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                logits = model(x)\n",
    "\n",
    "                # BCEWithLogitsLoss expects float (1.0, 0.0)\n",
    "                y = y.float().unsqueeze(1)\n",
    "                \n",
    "                current_cost = torch.nn.BCEWithLogitsLoss()(logits, y)\n",
    "\n",
    "                cost_accumulated += current_cost.item() * x.size(0)\n",
    "                n_samples += x.size(0)\n",
    "\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= 0.5).int()\n",
    "\n",
    "                y_true.extend(y.squeeze(1).int().numpy())\n",
    "                y_pred.extend(preds.numpy())\n",
    "\n",
    "        # Average test_cost\n",
    "        test_cost = cost_accumulated / n_samples\n",
    "\n",
    "        # Calculating the TN, FP, FN and TP\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Calculating sensitivity and specificity\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "\n",
    "        # Storing results\n",
    "        results.append({\n",
    "            \"padding\": pad,\n",
    "            \"stride\": stride,\n",
    "            \"test_cost\": test_cost,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"sensitivity\": sensitivity,\n",
    "            \"specificity\": specificity\n",
    "            })\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f1d1e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating labels\n",
    "padding_values = sorted(set(r[\"padding\"] for r in results))\n",
    "stride_values = sorted(set(r[\"stride\"] for r in results))\n",
    "\n",
    "# Initialize matrices\n",
    "cost_matrix_conv = np.zeros((len(padding_values), len(stride_values)))\n",
    "sensitivity_matrix_conv = np.zeros_like(cost_matrix_conv)\n",
    "specificity_matrix_conv = np.zeros_like(cost_matrix_conv)\n",
    "\n",
    "# Fill matrices\n",
    "for r in results:\n",
    "    i = padding_values.index(r[\"padding\"])\n",
    "    j = stride_values.index(r[\"stride\"])\n",
    "    \n",
    "    cost_matrix_conv[i, j] = r[\"test_cost\"] if \"test_cost\" in r else np.nan\n",
    "    sensitivity_matrix_conv[i, j] = r[\"sensitivity\"]\n",
    "    specificity_matrix_conv[i, j] = r[\"specificity\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c27068d0",
   "metadata": {},
   "source": [
    "#### Cost Analysis - Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61dc4237",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(cost_matrix_conv,annot=True,fmt=\".4f\",xticklabels=stride_values,yticklabels=padding_values,cmap=\"Greens\")\n",
    "plt.xlabel(\"Stride\")\n",
    "plt.ylabel(\"Padding\")\n",
    "plt.title(\"Binary Cross-Entropy - Conv Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f4a6ca",
   "metadata": {},
   "source": [
    "#### Sensitivity Analysis - Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0fc4dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(sensitivity_matrix_conv,annot=True,fmt=\".3f\",xticklabels=stride_values,yticklabels=padding_values,cmap=\"Greens\")\n",
    "plt.xlabel(\"Stride\")\n",
    "plt.ylabel(\"Padding\")\n",
    "plt.title(\"Sensitivity - Conv Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca44ec19",
   "metadata": {},
   "source": [
    "#### Specitivity Analysis - Convolutional Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9e1b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(specificity_matrix_conv,annot=True,fmt=\".3f\",xticklabels=stride_values,yticklabels=padding_values,cmap=\"Greens\")\n",
    "plt.xlabel(\"Stride\")\n",
    "plt.ylabel(\"Padding\")\n",
    "plt.title(\"Specificity - Conv Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb14ee5",
   "metadata": {},
   "source": [
    "### Hyperparameterizing max pooling kernel size and stride"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e9a11ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "pooling_kernel_sizes = [2,3,4,5]\n",
    "pooling_strides = [1,2,3,4]\n",
    "\n",
    "# Initialzing lists to store evaluation metrics across models\n",
    "pooling_results = []\n",
    "pooling_metrics = []\n",
    "\n",
    "# Iterating over the hyperparameters\n",
    "for pool_kernel in pooling_kernel_sizes:\n",
    "    for pool_stride in pooling_strides:\n",
    "\n",
    "\n",
    "        # Initializing CNN with hyperparameters\n",
    "        model = CNN(padding = 1, stride = 2, pool_kernel=pool_kernel, pool_stride=pool_stride)  \n",
    "        # Metrics from trained model\n",
    "        pooling_layer = train_model(model, train_loader, val_loader, num_epochs=10)\n",
    "\n",
    "        # Store metrics for the current hyperparameter combination\n",
    "        pooling_metrics.append({\"pool_kernel\": pool_kernel,\"pool_stride\": pool_stride, \"history\": pooling_layer})\n",
    "\n",
    "        # Evaluating the current hyperparameter combination\n",
    "        model.eval()\n",
    "\n",
    "        cost_accumulated = 0.0\n",
    "        n_samples = 0\n",
    "        y_true = []\n",
    "        y_pred = []\n",
    "\n",
    "        # Generating predictions on the test data \n",
    "        with torch.no_grad():\n",
    "            for x, y in test_loader:\n",
    "                logits = model(x)\n",
    "\n",
    "                # BCEWithLogitsLoss expects float 1, 0 --> 1.0, 0.0\n",
    "                y = y.float().unsqueeze(1)\n",
    "\n",
    "                current_cost = torch.nn.BCEWithLogitsLoss()(logits, y)\n",
    "\n",
    "                cost_accumulated += current_cost.item() * x.size(0)\n",
    "                n_samples += x.size(0)\n",
    "\n",
    "                probs = torch.sigmoid(logits)\n",
    "                preds = (probs >= 0.5).int()\n",
    "\n",
    "                y_true.extend(y.squeeze(1).int().numpy())\n",
    "                y_pred.extend(preds.numpy())\n",
    "\n",
    "        # Average test_cost\n",
    "        test_cost = cost_accumulated / n_samples\n",
    "\n",
    "        # Calculating the TN, FP, FN and TP\n",
    "        cm = confusion_matrix(y_true, y_pred)\n",
    "        tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "        # Calculating sensitivity and specificity\n",
    "        sensitivity = tp / (tp + fn)\n",
    "        specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "        # Storing results\n",
    "        pooling_results.append({\n",
    "            \"pool_kernel\": pool_kernel,\n",
    "            \"pool_stride\": pool_stride,\n",
    "            \"test_cost\": test_cost,\n",
    "            \"confusion_matrix\": cm,\n",
    "            \"sensitivity\": sensitivity,\n",
    "            \"specificity\": specificity\n",
    "        })\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e17f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generating labels\n",
    "pool_kernel_values = sorted(set(r[\"pool_kernel\"] for r in pooling_results))\n",
    "pool_stride_values = sorted(set(r[\"pool_stride\"] for r in pooling_results))\n",
    "\n",
    "# Formatting labels as \"2x2\", \"3x3\", ..., \"NxN\"\n",
    "pool_kernel_labels = [f\"{k}x{k}\" for k in pool_kernel_values]\n",
    "\n",
    "\n",
    "# Initialize matrices\n",
    "cost_matrix_pooling = np.full((len(pool_kernel_values), len(pool_stride_values)), np.nan)\n",
    "sensitivity_matrix_pooling = np.full_like(cost_matrix_pooling, np.nan)\n",
    "specificity_matrix_pooling = np.full_like(cost_matrix_pooling, np.nan)\n",
    "\n",
    "# Fill matrices\n",
    "for r in pooling_results:\n",
    "    i = pool_kernel_values.index(r[\"pool_kernel\"])\n",
    "    j = pool_stride_values.index(r[\"pool_stride\"])\n",
    "\n",
    "    cost_matrix_pooling[i, j] = r[\"test_cost\"]\n",
    "    sensitivity_matrix_pooling[i, j] = r[\"sensitivity\"]\n",
    "    specificity_matrix_pooling[i, j] = r[\"specificity\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f0d005",
   "metadata": {},
   "source": [
    "#### Cost Analysis - Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef937549",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(cost_matrix_pooling,annot=True,fmt=\".4f\",xticklabels=pool_stride_values, yticklabels=pool_kernel_labels,cmap=\"Greens\")\n",
    "plt.xlabel(\"Filter Stride\")\n",
    "plt.ylabel(\"Filter Size\")\n",
    "plt.title(\"Binary Cross-Entropy - Pooling Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5617900",
   "metadata": {},
   "source": [
    "#### Sensitivity Analysis - Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ce48141",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(sensitivity_matrix_pooling,annot=True,fmt=\".3f\",xticklabels=pool_stride_values,yticklabels=pool_kernel_labels,cmap=\"Greens\")\n",
    "plt.xlabel(\"Filter Stride\")\n",
    "plt.ylabel(\"Filter Size\")\n",
    "plt.title(\"Sensitivity - Pooling Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd98584",
   "metadata": {},
   "source": [
    "#### Specitivity Analysis - Pooling Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7875837",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "sns.heatmap(specificity_matrix_pooling,annot=True,fmt=\".3f\",xticklabels=pool_stride_values,yticklabels=pool_kernel_labels,cmap=\"Greens\")\n",
    "plt.xlabel(\"Filter Stride\")\n",
    "plt.ylabel(\"Filter Size\")\n",
    "plt.title(\"Specificity - Pooling Layer\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1deea61d",
   "metadata": {},
   "source": [
    "### Final model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "002035a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final parameters\n",
    "## Convolutional layer\n",
    "kernel_size_conv = 3\n",
    "stride_conv = 3\n",
    "padding_conv = 3\n",
    "\n",
    "\n",
    "## Pooling layer\n",
    "kernel_size_pooling = 3\n",
    "stride_pooling = 1\n",
    "\n",
    "final_results = []\n",
    "final_metrics = []\n",
    "\n",
    "# Initializing CNN with hyperparameters\n",
    "model = CNN(kernel_size= kernel_size_conv, padding = padding_conv, stride = stride_conv, pool_kernel=kernel_size_pooling, pool_stride=stride_pooling)  \n",
    "# Metrics from trained model\n",
    "final = train_model(model, train_loader, val_loader, num_epochs=15)\n",
    "\n",
    "# Store metrics for the current hyperparameter combination\n",
    "final_metrics.append({\"history\": final})\n",
    "\n",
    "# Evaluating the current hyperparameter combination\n",
    "model.eval()\n",
    "\n",
    "cost_accumulated = 0.0\n",
    "n_samples = 0\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "# Generating predictions on the test data \n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        logits = model(x)\n",
    "\n",
    "        # BCEWithLogitsLoss expects float (1.0, 0.0)\n",
    "        y = y.float().unsqueeze(1)\n",
    "\n",
    "        current_cost = torch.nn.BCEWithLogitsLoss()(logits, y)\n",
    "\n",
    "        cost_accumulated += current_cost.item() * x.size(0)\n",
    "        n_samples += x.size(0)\n",
    "\n",
    "        probs = torch.sigmoid(logits)\n",
    "        preds = (probs >= 0.5).int()\n",
    "\n",
    "        y_true.extend(y.squeeze(1).int().numpy())\n",
    "        y_pred.extend(preds.numpy())\n",
    "\n",
    "# Average test_cost\n",
    "test_cost = cost_accumulated / n_samples\n",
    "\n",
    "# Calculating the TN, FP, FN and TP\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Calculating sensitivity and specificity\n",
    "sensitivity = tp / (tp + fn)\n",
    "specificity = tn / (tn + fp)\n",
    "\n",
    "\n",
    "# Storing results\n",
    "final_results.append({\n",
    "    \"test_cost\": test_cost,\n",
    "    \"confusion_matrix\": cm,\n",
    "    \"sensitivity\": sensitivity,\n",
    "    \"specificity\": specificity\n",
    "})\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74eea050",
   "metadata": {},
   "source": [
    "#### Confusion Matrix, Performance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68084488",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "\n",
    "# Printing metrics\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Test Cost: {test_cost:.4f}\")\n",
    "print(f\"Sensitivity: {sensitivity:.4f}\")\n",
    "print(f\"Specificity: {specificity:.4f}\")\n",
    "\n",
    "\n",
    "plt.figure(figsize=(3.5, 2.5))\n",
    "ax = sns.heatmap(cm,annot=True,fmt=\"d\",cmap=\"Greens\",xticklabels=[\"No Pneumonia\", \"Pneumonia\"],yticklabels=[\"No Pneumonia\", \"Pneumonia\"])\n",
    "ax.xaxis.tick_top()\n",
    "ax.xaxis.set_label_position('top')\n",
    "ax.invert_yaxis()\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "209013c9",
   "metadata": {},
   "source": [
    "#### Model summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a80e5c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchinfo import summary\n",
    "# (batch, channels, height, width)\n",
    "summary(model,input_size=(32, 1,480, 480),col_names=[\"input_size\", \"output_size\", \"num_params\", \"kernel_size\"],depth=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ebecb94",
   "metadata": {},
   "source": [
    "#### Extracting feature maps from the convolutional layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90737d68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty dictionary to store feature maps from the convolutional layers\n",
    "feature_maps = {}\n",
    "\n",
    "\n",
    "def make_hook(name):\n",
    "    def hook(module, input, output):\n",
    "        feature_maps[name] = output.detach()\n",
    "    return hook\n",
    "\n",
    "h1 = model.conv1.register_forward_hook(make_hook(\"conv1\"))\n",
    "h2 = model.conv2.register_forward_hook(make_hook(\"conv2\"))\n",
    "h3 = model.conv3.register_forward_hook(make_hook(\"conv3\"))\n",
    "\n",
    "# Forward pass\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for x, y in test_loader:\n",
    "        x = x.to(next(model.parameters()).device)\n",
    "        _ = model(x)\n",
    "        break\n",
    "\n",
    "# Removing hook\n",
    "h1.remove()\n",
    "h2.remove()\n",
    "h3.remove()\n",
    "\n",
    "# Image index\n",
    "image = 9\n",
    "feature_idx = [0, 1, 2, 3]\n",
    "\n",
    "layers = [\"conv1\", \"conv2\", \"conv3\"]\n",
    "\n",
    "fig, axes = plt.subplots(len(layers), len(feature_idx), figsize=(4, 4))\n",
    "\n",
    "for row, layer in enumerate(layers):\n",
    "    fmap = feature_maps[layer][image] \n",
    "\n",
    "    for col, feature_index in enumerate(feature_idx):\n",
    "        fm = fmap[feature_index].cpu()\n",
    "        fm = (fm - fm.min()) / (fm.max() - fm.min() + 1e-8)\n",
    "\n",
    "        ax = axes[row, col]\n",
    "        ax.imshow(fm, cmap=\"viridis\")\n",
    "\n",
    "        H, W = fm.shape\n",
    "\n",
    "        ax.set_xticks([0, W//2, W-1])\n",
    "        ax.set_yticks([0, H//2, H-1])\n",
    "\n",
    "        ax.set_xticklabels([0, W//2, W-1], fontsize=7)\n",
    "        ax.set_yticklabels([0, H//2, H-1], fontsize=7)\n",
    "\n",
    "\n",
    "        if row == 0:\n",
    "            ax.set_title(f\"Feature {feature_index}\", fontsize=9)\n",
    "\n",
    "\n",
    "    axes[row, 0].set_ylabel(layer, fontsize=11, rotation=90, labelpad=3)\n",
    "\n",
    "    \n",
    "plt.subplots_adjust(wspace=0.4,   hspace=0.000)\n",
    "plt.suptitle(\"Feature Maps Across Convolutional Layers\", fontsize=12)\n",
    "plt.tight_layout\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f86666c",
   "metadata": {},
   "source": [
    "#### Overlaying features on the input image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f653271c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function that finds two images with (prob>0.9) and without pneumonia (prob<0.1). \n",
    "def image_search(model, test_loader, device=\"cpu\",pos_thresh=0.9, neg_thresh=0.1):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    pos_case = None\n",
    "    neg_case = None\n",
    "\n",
    "    # Forward pass with predictions\n",
    "    with torch.no_grad():\n",
    "        for x, y in test_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            logits = model(x)               \n",
    "            probs = torch.sigmoid(logits)    \n",
    "\n",
    "            for i in range(x.size(0)):\n",
    "                prob = probs[i].item()\n",
    "                label = y[i].item()\n",
    "\n",
    "                if pos_case is None and label == 1 and prob >= pos_thresh:\n",
    "                    pos_case = {\"img\": x[i:i+1].cpu(),\"label\": int(label),\"logit\": logits[i].item(),\"prob\": prob}\n",
    "\n",
    "                if neg_case is None and label == 0 and prob <= neg_thresh:\n",
    "                    neg_case = {\"img\": x[i:i+1].cpu(),\"label\": int(label),\"logit\": logits[i].item(),\"prob\": prob}\n",
    "\n",
    "                if pos_case is not None and neg_case is not None:\n",
    "                    return pos_case, neg_case\n",
    "\n",
    "    return pos_case, neg_case\n",
    "\n",
    "# Finding two images with and without pneumonia\n",
    "pos_case, neg_case = image_search(model,test_loader,device=\"cpu\",pos_thresh=0.8,neg_thresh=0.1)\n",
    "\n",
    "fig, axes = plt.subplots(1, 2)\n",
    "\n",
    "img  = pos_case[\"img\"]\n",
    "prob = pos_case[\"prob\"]\n",
    "\n",
    "cam = gradcam(model, img, conv_layer=model.conv3)\n",
    "\n",
    "xray = img.squeeze()\n",
    "xray = (xray - xray.min()) / (xray.max() - xray.min() + 1e-8)\n",
    "\n",
    "axes[0].imshow(xray, cmap=\"gray\")\n",
    "axes[0].imshow(cam, alpha=0.3)\n",
    "axes[0].axis(\"off\")\n",
    "axes[0].set_title(f\"Pneumonia\\nProbability: {prob:.2f}\")\n",
    "\n",
    "\n",
    "img  = neg_case[\"img\"]\n",
    "prob = neg_case[\"prob\"]\n",
    "\n",
    "cam = gradcam(model, img, conv_layer=model.conv3)\n",
    "\n",
    "xray = img.squeeze()\n",
    "xray = (xray - xray.min()) / (xray.max() - xray.min() + 1e-8)\n",
    "\n",
    "axes[1].imshow(xray, cmap=\"gray\")\n",
    "axes[1].imshow(cam, alpha=0.3)\n",
    "axes[1].axis(\"off\")\n",
    "axes[1].set_title(f\"No Pneumonia\\nProbability: {prob:.2f}\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.11.4)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
