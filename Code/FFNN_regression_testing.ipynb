{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b44b34c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from FFNN import NeuralNetwork\n",
    "from activation_functions import ReLU, leaky_ReLU, sigmoid, mse\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcc96cd3",
   "metadata": {},
   "source": [
    "### Generate data\n",
    "Using the same function and seed for generating data as in project 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "765dd815",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data(n, noise = True):\n",
    "    # Fixed seed value to ensure consisten results across runs\n",
    "    seed = 42\n",
    "    np.random.seed(seed)\n",
    "    # Creating an array with equally spaced data\n",
    "    x = np.linspace(-1, 1, n)\n",
    "    # Runges equation with noise\n",
    "    if noise:\n",
    "        y = 1 / (1 + 25*x**2) +  np.random.normal(0, 0.1, size=n)\n",
    "    else:\n",
    "        y = 1 / (1 + 25*x**2)\n",
    "    return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "89a295f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "x, y = generate_data(50)\n",
    "x = x.reshape(-1,1)\n",
    "y = y.reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "cbc98b9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 2️⃣ Split into training and test sets\n",
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    x, y, test_size=0.2, random_state=42\n",
    ")\n",
    "\n",
    "# 3️⃣ Create a validation set from the training data\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_full, y_train_full, test_size=0.25, random_state=42\n",
    ")\n",
    "# (→ 60% train, 20% val, 20% test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "b43026b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4️⃣ Scale features\n",
    "scaler = StandardScaler()\n",
    "x_train = scaler.fit_transform(X_train)\n",
    "x_val   = scaler.transform(X_val)\n",
    "x_test  = scaler.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7433a31d",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'layer' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[29]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# 5️⃣ Initialize the neural network\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m nn = \u001b[43mNeuralNetwork\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnetwork_input_size\u001b[49m\u001b[43m=\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m.\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlayer_output_sizes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mactivation_funcs\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mReLU\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigmoid\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcost_func\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmse\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# 6️⃣ Train using your chosen optimizer\u001b[39;00m\n\u001b[32m      9\u001b[39m nn.train_network(nn, X_train, y_train, eta=\u001b[32m0.01\u001b[39m, epochs=\u001b[32m100\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Skole\\Anvendt dataanalyse og Maskinlæring\\Project2\\Project2_FYSSTK3155-4155\\Code\\FFNN.py:20\u001b[39m, in \u001b[36mNeuralNetwork.__init__\u001b[39m\u001b[34m(self, network_input_size, layer_output_sizes, activation_funcs, cost_func)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28mself\u001b[39m.activation_funcs = activation_funcs\n\u001b[32m     19\u001b[39m \u001b[38;5;28mself\u001b[39m.cost_func = cost_func\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m \u001b[38;5;28mself\u001b[39m.layers = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_create_layers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     21\u001b[39m \u001b[38;5;28mself\u001b[39m.gradient_func = \u001b[38;5;28mself\u001b[39m._create_gradient_func()\n\u001b[32m     22\u001b[39m \u001b[38;5;28mself\u001b[39m.velocities = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Skole\\Anvendt dataanalyse og Maskinlæring\\Project2\\Project2_FYSSTK3155-4155\\Code\\FFNN.py:51\u001b[39m, in \u001b[36mNeuralNetwork._create_layers\u001b[39m\u001b[34m(self, batch_norm)\u001b[39m\n\u001b[32m     49\u001b[39m         running_mean = np.zeros((\u001b[32m1\u001b[39m, layer_output_size))\n\u001b[32m     50\u001b[39m         running_var = np.ones((\u001b[32m1\u001b[39m, layer_output_size))\n\u001b[32m---> \u001b[39m\u001b[32m51\u001b[39m         \u001b[43mlayer\u001b[49m.update({\u001b[33m'\u001b[39m\u001b[33mgamma\u001b[39m\u001b[33m'\u001b[39m: gamma, \u001b[33m'\u001b[39m\u001b[33mbeta\u001b[39m\u001b[33m'\u001b[39m: beta,\u001b[33m'\u001b[39m\u001b[33mrunning_mean\u001b[39m\u001b[33m'\u001b[39m: running_mean,\u001b[33m'\u001b[39m\u001b[33mrunning_var\u001b[39m\u001b[33m'\u001b[39m: running_var,})\n\u001b[32m     53\u001b[39m \u001b[38;5;66;03m# Return the list of weights and biases for each layer\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m layers\n",
      "\u001b[31mNameError\u001b[39m: name 'layer' is not defined"
     ]
    }
   ],
   "source": [
    "# 5️⃣ Initialize the neural network\n",
    "nn = NeuralNetwork(\n",
    "    network_input_size=X_train.shape[1],\n",
    "    layer_output_sizes=[50, 100, 1],\n",
    "    activation_funcs=[ReLU, sigmoid],\n",
    "    cost_func=mse)\n",
    "\n",
    "# 6️⃣ Train using your chosen optimizer\n",
    "nn.train_network(nn, X_train, y_train, eta=0.01, epochs=100)\n",
    "\n",
    "# 7️⃣ Monitor validation performance\n",
    "y_val_pred = nn.predict(X_val)\n",
    "val_loss = mse(y_val, y_val_pred)\n",
    "print(\"Validation loss:\", val_loss)\n",
    "\n",
    "# 8️⃣ Evaluate on the test set (only once, after final tuning)\n",
    "y_test_pred = nn.predict(X_test)\n",
    "test_loss = mse(y_test, y_test_pred)\n",
    "print(\"Final test loss:\", test_loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.0)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
